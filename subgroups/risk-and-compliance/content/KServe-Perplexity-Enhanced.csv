"Risk_Lifecycle","Main_Risk","Sub_Risk","Rule_ID","Rule_Title","Rule_Description","NIST_AI_RMF_Function","NIST_AI_RMF_Details","Associated_Metric","Metric_Calculation","HF_Metric","Metric_Based_Rule","Threshold_Value","HF_Metric_Match","Mapping_Notes"
"Training Data","Accuracy","Data contamination","KSERVE-1.1.1","Ensure model input data validation is implemented","Implement comprehensive input validation for training data loaded into KServe models to prevent contaminated or malicious data from being used. Validate data schema, ranges, types, and content against defined specifications before model training or serving.","MEASURE-2.2","Evaluations involving human subjects meet applicable requirements and are representative of the relevant population","Data validation pass rate, Schema conformance percentage, Contamination detection rate","Count of validation checks passed / Total validation checks performed. Monitor for schema violations and data type mismatches in input pipelines.","Data validation pass rate","Ensure data validation pass rate is greater than threshold","95%","Yes","HF models track schema conformance and contamination detection rates"
"Training Data","Accuracy","Unrepresentative data","KSERVE-1.1.2","Ensure model evaluation includes representative population segments","Configure KServe model evaluation and monitoring to assess performance across different demographic segments and use cases to ensure the model is representative of the intended population. Use stratified evaluation approaches.","MEASURE-2.2","Evaluations involving human subjects meet applicable requirements and are representative of the relevant population","Population segment coverage percentage, Model performance variance across segments, Evaluation data diversity score","Count of evaluated population segments / Total identified segments. Calculate performance metrics separately for each demographic group.","Population segment coverage percentage","Ensure population segment coverage is greater than threshold","90%","Yes","Model cards document evaluation across demographic segments"
"Training Data","Privacy","Personal information in data","KSERVE-1.2.1","Ensure PII redaction in model training pipelines","Implement automated PII (Personally Identifiable Information) detection and redaction in data preprocessing steps before models are trained or stored in KServe. Use techniques such as tokenization, masking, or differential privacy.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","PII detection rate, Data anonymization coverage, Residual PII percentage","Count of PII instances detected and redacted / Total records processed. Verify through automated scanning and manual audit sampling.","Residual PII percentage","Ensure residual PII percentage is less than threshold","1%","Yes","PII detection models (e.g., Piiranha) document PII detection rates"
"Training Data","Privacy","Reidentification risk","KSERVE-1.2.2","Ensure model inference does not enable re-identification attacks","Apply privacy-preserving techniques such as k-anonymity, l-diversity, or differential privacy to model training data to prevent re-identification of individuals. Document privacy guarantees and re-identification risk assessments.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","Re-identification risk score, Privacy guarantee level, Differential privacy budget utilization","Conduct re-identification attacks using standard benchmarks. Measure successful re-identifications / Total test cases. Document epsilon value for differential privacy.","Membership inference attack success rate","Ensure membership inference attack success rate is less than threshold","5%","Yes","Privacy models track attack success rates"
"Training Data","Transparency","Lack of training data transparency","KSERVE-1.3.1","Ensure training data documentation and versioning","Maintain comprehensive documentation of all training datasets used by KServe models including data source, collection methodology, preprocessing steps, labeling procedures, and version control. Store documentation with model artifacts.","GOVERN-1.6","Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities","Documentation completeness score, Dataset versioning compliance, Metadata availability percentage","Evaluate documentation against checklist of required elements (source, collection date, preprocessing, labeling). Verify all training datasets have unique versions tracked in model registry.","Documentation completeness score","N/A - Procedural Rule","N/A","No","Process/procedural rule - documentation completeness is not a quantifiable metric in model cards"
"Training Data","Transparency","Uncertain data provenance","KSERVE-1.3.2","Ensure data provenance tracking and validation","Implement mechanisms in KServe data pipelines to track and verify the provenance of all training and validation datasets. Use data lineage tools and checksums to ensure data integrity and authenticity.","GOVERN-1.6","Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities","Data provenance tracking coverage, Checksum verification pass rate, Data source authenticity score","Count of datasets with verified provenance / Total datasets. Verify checksums match source repositories. Document chain of custody for all data transformations.","Data provenance tracking coverage","N/A - Procedural Rule","N/A","No","Process/procedural rule - data provenance tracking is not a standard model card metric"
"Inference","Accuracy","Poor model accuracy","KSERVE-2.1.1","Ensure continuous model performance monitoring","Deploy monitoring for KServe model inference to continuously track accuracy metrics (precision, recall, F1-score, etc.) against baseline performance. Trigger automated alerts when performance degrades below acceptable thresholds.","MEASURE-2.2","Evaluations involving human subjects meet applicable requirements and are representative of the relevant population","Model accuracy baseline, Performance degradation threshold breaches, Alert response time","Calculate precision, recall, F1-score on inference outputs. Compare against baseline; trigger alert if falls below defined threshold (e.g., 5% degradation).","Model accuracy (F1-score, precision, recall)","Ensure model accuracy baseline is greater than threshold","90%","Yes","Standard metrics in HF model cards via HELM and evaluation results"
"Inference","Accuracy","Poor model accuracy","KSERVE-2.1.2","Ensure data drift detection and model retraining triggers","Configure KServe with data drift detection mechanisms to identify when inference input distributions change significantly from training data. Automatically trigger model retraining or alerting when drift exceeds thresholds.","MEASURE-2.2","Evaluations involving human subjects meet applicable requirements and are representative of the relevant population","Data drift detection sensitivity, Retraining trigger frequency, Model staleness metric","Use statistical tests (KL divergence, Kolmogorov-Smirnov) to measure distribution changes. Track days since last model retraining. Alert if drift score exceeds threshold.","Data drift detection score","N/A - Procedural Rule","N/A","No","Process/procedural rule - drift detection is operational metric, not model card metric"
"Inference","Intellectual Property","Confidential data in prompt/inference","KSERVE-2.2.1","Ensure input sanitization and confidential data redaction","Implement input sanitization and redaction mechanisms in KServe inference pipelines to automatically detect and remove or mask confidential data (API keys, credentials, PII) from inference requests before they reach the model.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","Confidential data detection rate, Redaction coverage percentage, False positive rate","Run inference requests through redaction engine; measure percentage of confidential patterns detected. Monitor redaction false positives.","Redaction coverage percentage","Ensure redaction coverage is greater than threshold","98%","Yes","PII/sensitive data redaction metrics documented in model cards"
"Inference","Privacy","Attribute inference attack","KSERVE-2.3.1","Ensure protection against attribute inference attacks","Implement defenses in KServe against attribute inference attacks where attackers query the model multiple times to infer sensitive training data attributes. Apply differential privacy, output perturbation, or query rate limiting.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","Query rate limit enforcement, Inference output perturbation magnitude, Attack success rate","Monitor inference request patterns; block/throttle clients exceeding rate limits. Measure attacker success rate in inferring attributes from model outputs.","Attribute inference attack success rate","Ensure attribute inference attack success rate is less than threshold","5%","Yes","DecodingTrust and similar benchmarks evaluate privacy metrics"
"Inference","Privacy","Membership inference attack","KSERVE-2.3.2","Ensure protection against membership inference attacks","Protect KServe models against membership inference attacks where attackers determine if specific samples were used in training. Apply differential privacy, gradient clipping, or confidence calibration techniques.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","Membership inference attack success rate, Model confidence calibration score, Differential privacy epsilon value","Conduct membership inference attacks using standard benchmarks. Measure accuracy of determining training data membership. Apply differential privacy with bounded epsilon values.","Membership inference attack success rate","Ensure membership inference attack success rate is less than threshold","5%","Yes","Privacy evaluation metric from HELM Safety and DecodingTrust"
"Inference","Privacy","Personal information in prompt","KSERVE-2.3.3","Ensure PII handling in inference requests and logging","Configure KServe logging and monitoring to redact PII from inference requests and model outputs. Ensure inference logs do not contain sensitive personal information and are retained according to compliance requirements.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","PII redaction coverage in logs, Log retention compliance rate, Sensitive data exposure incidents","Audit logs for PII presence using automated scanning. Verify log retention matches compliance policy (GDPR, CCPA, etc.). Track data exposure incidents.","PII redaction coverage in logs","Ensure PII redaction coverage in logs is greater than threshold","99%","Yes","Privacy metrics tracked in model card evaluation results"
"Inference","Robustness - Model Behavior","Evasion attack","KSERVE-2.4.1","Ensure adversarial robustness testing of inference models","Conduct regular adversarial robustness testing on KServe models using evasion attacks (adversarial examples, input perturbations). Maintain test results and implement mitigations for identified vulnerabilities.","MEASURE-2.5","The AI system to be deployed is demonstrated to be valid and reliable","Evasion attack success rate, Adversarial robustness score, Model stability index","Generate adversarial examples using techniques (FGSM, PGD, C&W). Measure attack success rate. Calculate robustness metrics on adversarial test sets.","Adversarial robustness score","Ensure adversarial robustness score is greater than threshold","85%","Yes","HELM Safety and robustness benchmarks measure adversarial success rates"
"Inference","Robustness - Model Behavior","Extraction attack","KSERVE-2.4.2","Ensure protection against model extraction attacks","Implement mechanisms to prevent model extraction attacks in KServe where attackers query the model to build surrogate models. Use output perturbation, prediction confidence limits, or query rate limiting.","MEASURE-2.5","The AI system to be deployed is demonstrated to be valid and reliable","Surrogate model fidelity threshold, Query rate limits, API response perturbation magnitude","Conduct extraction attacks attempting to build surrogate models. Measure fidelity of stolen models. Monitor query patterns for suspicious sampling behavior.","Model extraction attack success rate","Ensure model extraction attack success rate is less than threshold","20%","Yes","Robustness metrics in model card evaluations"
"Inference","Robustness - Prompt Attacks","Prompt injection attack","KSERVE-2.5.1","Ensure input validation against prompt injection attacks","Implement prompt injection detection and mitigation in KServe inference pipelines to prevent malicious prompt structures from manipulating model behavior. Validate and sanitize all user inputs before model inference.","MEASURE-2.5","The AI system to be deployed is demonstrated to be valid and reliable","Prompt injection detection rate, Input validation pass rate, Malicious prompt block rate","Test with known prompt injection patterns. Measure detection accuracy. Calculate percentage of suspicious prompts blocked versus passed.","Prompt injection detection rate","Ensure prompt injection detection rate is greater than threshold","95%","Yes","Safety benchmarks include prompt injection detection"
"Inference","Robustness - Prompt Attacks","Direct instructions attack","KSERVE-2.5.2","Ensure model guardrails and instruction filtering","Deploy instruction filtering and model guardrails in KServe to prevent direct harmful instructions from being executed by the model. Maintain allowlists/denylists of prohibited operations and monitor for attempts to bypass guardrails.","MEASURE-2.5","The AI system to be deployed is demonstrated to be valid and reliable","Guardrail bypass attempt rate, Blocked harmful instruction percentage, Guardrail effectiveness score","Test model with direct harmful instructions. Measure percentage successfully blocked. Track guardrail bypass attempts and adjust rules.","Blocked harmful instruction percentage","Ensure blocked harmful instruction percentage is greater than threshold","95%","Yes","HELM Safety covers harmful request filtering"
"Inference","Robustness - Prompt Attacks","Prompt leaking / System prompt extraction","KSERVE-2.5.3","Ensure system prompt/configuration is not exposed","Configure KServe models to prevent exposure of internal system prompts, configuration details, or model parameters through inference APIs. Implement response filtering and access controls.","MEASURE-2.5","The AI system to be deployed is demonstrated to be valid and reliable","System prompt exposure incidents, Response content filtering effectiveness, Unauthorized access attempts blocked","Conduct prompt leaking attacks. Monitor for system prompt disclosure in responses. Track unauthorized access attempts to model internals.","System prompt exposure incidents","Ensure system prompt exposure incidents are less than threshold","0","Yes","Security metric tracked in robustness evaluations"
"Output","Explainability","Unexplainable output","KSERVE-3.1.1","Ensure model explainability implementation","Deploy explainability mechanisms for KServe models to provide interpretable explanations for model predictions. Implement techniques such as LIME, SHAP, or attention mechanisms. Document model decision factors for users.","MEASURE-2.13","Effectiveness of employed TEVV metrics and processes are evaluated and documented","Explainability coverage percentage, Explanation consistency score, Feature importance stability","Calculate percentage of predictions with explanations generated. Measure consistency of explanations across similar inputs. Track feature importance stability.","Explainability coverage percentage","Ensure explainability coverage is greater than threshold","90%","Yes","Model cards document explanation mechanisms and coverage"
"Output","Intellectual Property","Revealing confidential information","KSERVE-3.2.1","Ensure model output does not leak confidential data","Implement mechanisms in KServe to prevent model outputs from revealing confidential training data, API credentials, or sensitive information. Filter model outputs and log suspicious disclosures.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","Confidential data leak incidents, Output filtering effectiveness, Disclosure detection rate","Monitor model outputs for confidential patterns. Count disclosure incidents. Measure false positive rate of filtering mechanisms.","Confidential data leak incidents","Ensure confidential data leak incidents are less than threshold","0","Yes","Safety metric tracked in model evaluations"
"Output","Misuse","Improper usage","KSERVE-3.3.1","Ensure model usage is aligned with intended purpose","Document and enforce the intended use cases and constraints for KServe models. Monitor usage patterns and alert when models are used outside defined scope. Implement access controls based on use case.","GOVERN-1.7","Processes and procedures are in place for decommissioning and phasing out AI systems safely","Usage scope compliance rate, Out-of-scope request detection rate, Authorized user percentage","Define intended use cases and track requests by use case. Measure percentage of requests within scope. Monitor for unusual usage patterns.","Usage scope compliance rate","N/A - Procedural Rule","N/A","No","Process/procedural rule - usage scope compliance is operational metric"
"Output","Misuse","Non-disclosure of AI-generated content","KSERVE-3.3.2","Ensure AI-generated content is properly disclosed","Configure KServe models to clearly mark or disclose that content was generated by AI systems. Implement logging and monitoring to verify disclosure compliance in downstream systems.","GOVERN-1.7","Processes and procedures are in place for decommissioning and phasing out AI systems safely","AI disclosure compliance rate, Undisclosed output incidents, Disclosure verification success rate","Track percentage of outputs with proper AI disclosure. Monitor for undisclosed AI content in downstream systems. Measure disclosure verification effectiveness.","AI disclosure compliance rate","N/A - Procedural Rule","N/A","No","Process/procedural rule - disclosure compliance is not a model card metric"
"Output","Privacy","Exposing personal information","KSERVE-3.4.1","Ensure model outputs do not expose personal information","Configure KServe output filtering to prevent model outputs from exposing PII or personal information from training data. Use redaction, pattern matching, and content filtering techniques.","GOVERN-1.1","Legal and regulatory requirements involving AI are understood, managed, and documented","PII exposure incidents in outputs, Output filtering effectiveness, Privacy violation detection rate","Scan model outputs for PII patterns. Count exposure incidents. Measure filtering false positive/negative rates.","PII exposure incidents in outputs","Ensure PII exposure incidents in outputs is less than threshold","0.1%","Yes","Privacy metrics from model card evaluations"
"Output","Robustness","Hallucination","KSERVE-3.5.1","Ensure hallucination detection and mitigation","Implement mechanisms in KServe to detect and mitigate model hallucinations (factually false outputs). Use consistency checking, confidence scores, and fact-checking integrations. Alert users to potential hallucinations.","MEASURE-2.5","The AI system to be deployed is demonstrated to be valid and reliable","Hallucination detection rate, False information reduction percentage, Fact-check accuracy","Evaluate outputs against ground truth. Calculate hallucination percentage. Measure effectiveness of hallucination detection mechanisms.","Hallucination detection rate","Ensure hallucination detection rate is greater than threshold","90%","Yes","Robustness metric - TruthfulQA and fact-checking benchmarks"
"Output","Value Alignment","Over-reliance on model","KSERVE-3.6.1","Ensure model confidence scores and uncertainty quantification","Configure KServe models to provide confidence scores or uncertainty estimates with predictions. Educate users on proper model reliance through documentation and calibration metrics.","GOVERN-1.2","The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices","Confidence score availability rate, Calibration error, User reliance appropriateness score","Verify all predictions include confidence scores. Measure calibration between confidence and accuracy. Survey/measure user reliance appropriateness.","Calibration error","Ensure calibration error is less than threshold","0.15","Yes","HELM tracks calibration as key safety metric"
"Non-Technical","Governance","Incomplete usage definition","KSERVE-4.1.1","Ensure model usage documentation is complete","Maintain comprehensive documentation for all KServe models defining intended use cases, constraints, limitations, and appropriate user groups. Store in model registry and keep updated.","GOVERN-1.4","The risk management process and its outcomes are established through transparent policies, procedures, and other controls","Documentation completeness score, Usage definition coverage, Model registry documentation audit pass rate","Evaluate documentation against checklist of required elements. Calculate percentage of models with complete usage documentation in registry.","Documentation completeness score","N/A - Procedural Rule","N/A","No","Process/procedural rule - governance documentation is not a model card metric"
"Non-Technical","Governance","Incorrect risk testing","KSERVE-4.1.2","Ensure comprehensive AI risk testing and validation","Define and implement comprehensive testing procedures for KServe models covering accuracy, fairness, robustness, privacy, and explainability. Use appropriate metrics and maintain test coverage records.","GOVERN-1.3","Processes, procedures, and practices are in place to determine needed level of risk management activities","Risk test coverage percentage, Test suite completeness, Risk scenario coverage","Calculate percentage of identified risks covered by tests. Evaluate against risk taxonomy. Track test pass rates by risk category.","Risk test coverage percentage","N/A - Procedural Rule","N/A","No","Process/procedural rule - test coverage is operational, not model card metric"
"Non-Technical","Governance","Lack of data transparency","KSERVE-4.1.3","Ensure data transparency documentation in model metadata","Maintain transparent documentation of training data characteristics in KServe model metadata including dataset size, sources, collection dates, labeling methodology, and known biases.","GOVERN-1.4","The risk management process and its outcomes are established through transparent policies, procedures, and other controls","Data transparency documentation score, Metadata completeness percentage, Data documentation audit pass rate","Audit model metadata for required data documentation elements. Calculate percentage of complete data transparency documentation.","Data transparency documentation score","N/A - Procedural Rule","N/A","No","Process/procedural rule - documentation is not a quantifiable model card metric"
"Non-Technical","Governance","Lack of model transparency","KSERVE-4.1.4","Ensure model transparency and versioning in registry","Maintain model registry for all KServe deployments with complete documentation of model architecture, training procedures, performance metrics, deployment history, and known limitations.","GOVERN-1.6","Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities","Model registry completeness score, Version tracking accuracy, Documentation availability percentage","Audit registry for required model documentation. Verify all model versions are tracked. Calculate documentation coverage percentage.","Model registry completeness score","N/A - Procedural Rule","N/A","No","Process/procedural rule - registry completeness is not a model card metric"
"Non-Technical","Governance","Lack of system transparency","KSERVE-4.1.5","Ensure system architecture and model deployment documentation","Document KServe deployment architecture including model serving configuration, data pipelines, monitoring setup, and system interactions. Maintain as-deployed documentation and update when changes occur.","GOVERN-1.4","The risk management process and its outcomes are established through transparent policies, procedures, and other controls","Architecture documentation completeness, System component inventory accuracy, Deployment configuration audit pass rate","Evaluate system documentation against KServe deployment checklist. Verify all components are documented and tracked.","Architecture documentation completeness","N/A - Procedural Rule","N/A","No","Process/procedural rule - system documentation is not a model card metric"
"Non-Technical","Governance","Lack of testing diversity","KSERVE-4.1.6","Ensure diverse testing team and methodologies","Implement KServe model testing using diverse perspectives and methodologies including technical testing, fairness testing, security testing, and business logic validation. Document testing approach and include diverse testers.","GOVERN-1.4","The risk management process and its outcomes are established through transparent policies, procedures, and other controls","Testing diversity score, Perspective coverage percentage, Tester diversity metrics","Measure number of different testing domains covered. Calculate percentage of identified bias sources tested. Track diversity of testing team.","Testing diversity score","N/A - Procedural Rule","N/A","No","Process/procedural rule - testing diversity is not a model card metric"
"Non-Technical","Governance","Unrepresentative risk testing","KSERVE-4.1.7","Ensure representative testing with production-like data","Configure KServe model testing to use data representative of production environments and use cases. Track test data characteristics and compare to production data distributions.","GOVERN-1.4","The risk management process and its outcomes are established through transparent policies, procedures, and other controls","Test data representativeness score, Distribution matching percentage, Production-test data similarity metric","Compare test data characteristics to production data using statistical measures (KL divergence, etc.). Calculate similarity scores for key data dimensions.","Test data representativeness score","N/A - Procedural Rule","N/A","No","Process/procedural rule - data representativeness is operational metric"
